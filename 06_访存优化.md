# 访存优化

## Intro

在linux上测试将大小为1 << 27的vector所有元素设置为0 , 比设置为 1 了快了一倍,为什么捏

## 内存带宽

### cpu-bound 和 memory-bound

以上次的sin代码为例

```cpp
int main() {
    size_t n = 1<<26;
    std::vector<float> a(n);

    tbb::parallel_for(tbb::blocked_range<size_t>(0, n),
    [&] (tbb::blocked_range<size_t> r) {
        for (size_t i = r.begin(); i < r.end(); i++) {
            a[i] = std::sin(i);
        }
    });

    return 0;
}
```

通常来说,并行**能加速计算的部分,不能加速内存读写的部分**

* 对于需要泰勒展开计算,每次迭代计算量很大的sin代码 ,并行计算能有良好的效果,成为计算瓶颈

* 对于vector填充这种没有计算量,只有访存的循环体,并没有加速效果,称为**内存瓶颈**(memory-bound)

并行能减轻计算瓶颈,但是不能减轻内存瓶颈,所以后者是优化的重点

### 浮点加法的计算量

冷知识：并行地给浮点数组每个元素做一次加法反而更慢。

因为一次**浮点加法的计算量**和**访存的超高延迟**相比实在太少了。

计算太简单，数据量又大，并行只带来了多线程调度的额外开销。

经验公式：1次浮点读写 ≈ 8次浮点加法

如果矢量化成功（SSE）：1次浮点读写 ≈ 32次浮点加法

如果CPU有4核且矢量化成功：1次浮点读写 ≈ 128次浮点加法

### 常见操作花费时间

图中加法(add)和乘法(mul)都指的整数 , 浮点的乘法和加法速度基本上是相同的

L1/2/3 read和Main RAM read的时间指的是读一个缓存行（64字节）所花费的时间。

根据计算：125/64*4≈8

即从主内存读取一次float花费8个cycle，符合经验公式。

“right”和“wrong”指的是分支预测是否成功。

![image-20220228052337495](.\img\image-20220228052337495.png)

**结论** : 要想利用全部CPU核心,避免mem-bound, 需要func中有足够的计算量

当核心数量越多,CPU计算能力越强,相对之下来不及从内存读写数据,从而**越容易mem-bound**

### 内存

通过dmidecode工具可以查看当前设备的内存信息

如下图

![image-20220228053225122](.\img\image-20220228053225122.png)

可以看到这台设备上有两根内存条,频率都是2667MHz , 数据的宽度是64位 ( 8 字节)

理论极限带宽 = 频率 * 宽度 * 数量

​						= 2667 * 8 * 2 = 42672 MB/s

在频率相同的情况下 , 两根8GB的内存会比一根16GB的内存速度更快

这是因为系统会在两者之间吧读写均匀分配到两个内存上   实现内存的并行读写 , 这与磁盘RAID有一定相似之处

## 缓存和局域性

通过测试不同数据大小情况下的vector填充,我们会发现数据量较小时,实际带宽甚至超越了理论带宽42672MB/s

而数据量足够大时,才落回正常的带宽

![image-20220228054227428](.\img\image-20220228054227428.png)

![image-20220228054233255](.\img\image-20220228054233255.png)

### CPU内部的高速缓存

原来CPU的厂商早就意识到了内存延迟高，读写效率低下的问题。因此他们在CPU内部引入了一片极小的存储器——虽然小，但是读写速度却特别快。这片小而快的存储器称为缓存（cache）。

当CPU访问某个地址时，会先查找缓存中是否有对应的数据。如果没有，则从内存中读取，并存储到缓存中；如果有，则直接使用缓存中的数据。

这样一来，访问的数据量比较小时，就可以自动预先加载到这个更高效的缓存里，然后再开始做运算，从而避免从外部内存读写的超高延迟

### 高速缓存的分级结构

通过lscpu工具,我们可以查看cpu的告诉缓存大小

![image-20220228054653719](.\img\image-20220228054653719.png)

可以看到x86计算机的缓存分为三级

**一级缓存**分为**数据缓存**和**指令缓存**，其中数据缓存有 32 KB，6 个物理核心每个都有一个，总共 192 KB。而指令缓存的大小刚好和数据缓存一样也是 192 KB。

**二级缓存**有 256 KB，6 个物理核心每个都有一个，总共 1.5 MB。

**三级缓存**由各个物理核心共享，总共 12 MB。

![image-20220228054758787](.\img\image-20220228054758787.png)

### 图表分析

可以看到刚刚两个出现转折的点，也是在二级缓存和三级缓存的大小附近。

因此，数据小到装的进二级缓存，则最大带宽就取决于二级缓存的带宽。稍微大一点则只能装到三级缓存，就取决于三级缓存的带宽。三级缓存也装不下，那就取决于主内存的带宽了。

结论：要避免mem-bound，数据量尽量足够小，如果能装的进缓存就高效了

![image-20220228054918395](.\img\image-20220228054918395.png)

### 缓存的工作机制

* 当CPU读取一个地址时：

  缓存会查找和该地址匹配的条目。如果找到，则给CPU返回缓存中的数据。如果找不到，则向主内存发送请求，等读取到该地址的数据，就创建一个新条目。

  在 x86 架构中每个条目的存储 64 字节的数据，这个条目又称之为缓存行（cacheline）。

  当访问 0x0048~0x0050 这 4 个字节时，实际会导致 0x0040~0x0080 的 64 字节数据整个被读取到缓存中。

  这就是为什么我们喜欢把数据结构的起始地址和大小对齐到 64 字节，为的是不要浪费缓存行的存储空间。



* 当CPU写入一个地址时：

  缓存会查找和该地址匹配的条目。如果找到，则修改缓存中该地址的数据。如果找不到，则创建一个新条目来存储CPU写的数据，并标记为脏（dirty）。

  当读和写创建的新条目过多，缓存快要塞不下时，他会把最不常用的那个条目移除，这个现象称为失效（invalid）。如果那个条目是被标记为脏的，则说明是当时打算写入的数据，那就需要向主内存发送写入请求，等他写入成功，才能安全移除这个条目。

  如有多级缓存，则一级缓存失效后会丢给二级缓存。

### 连续访问和跨步访问

如果访问数组时，按一定的间距跨步访问，则效率如何？

从1到16都是一样快的，32开始才按2的倍率变快，为什么？

因为CPU和内存之间隔着缓存，而缓存和内存之间传输数据的最小单位是缓存行（64字节）。16个float是64字节，所以小于64字节的跨步访问，都会导致数据全部被读取出来。而超过64字节的跨步，则中间的缓存行没有被读取，从而变快了。

![image-20220228055829155](.\img\image-20220228055829155.png)

![image-20220228060018578](.\img\image-20220228060018578.png)

结论：访问内存的用时，和访问的字节数量无关，和访问的每个字节所在的缓存行数量有关。

可见，能否很好的利用缓存，和程序访问内存的**空间局域性**有关 , 在设计数据结构时，应该把数据存储的尽可能紧凑，不要松散排列。最好每个缓存行里要么有数据，要么没数据，避免读取缓存行时浪费一部分空间没用。

## 预取和直写

### 顺序访问和随机访问

```cpp
#include <iostream>
#include <vector>
#include <cmath>
#include <cstring>
#include <cstdlib>
#include <array>
#include <benchmark/benchmark.h>
#include <x86intrin.h>
#include <omp.h>

// L1: 32KB
// L2: 256KB
// L3: 12MB

constexpr size_t n = 1<<27;  // 512MB

std::vector<float> a(n);

void BM_ordered(benchmark::State &bm) {
    for (auto _: bm) {
#pragma omp parallel for
        for (size_t i = 0; i < n; i++) {
            benchmark::DoNotOptimize(a[i]);
        }
        benchmark::DoNotOptimize(a);
    }
}
BENCHMARK(BM_ordered);

static uint32_t randomize(uint32_t i) {
	i = (i ^ 61) ^ (i >> 16);
	i *= 9;
	i ^= i << 4;
	i *= 0x27d4eb2d;
	i ^= i >> 15;
    return i;
}

void BM_random(benchmark::State &bm) {
    for (auto _: bm) {
#pragma omp parallel for
        for (size_t i = 0; i < n; i++) {
            size_t r = randomize(i) % n;
            benchmark::DoNotOptimize(a[r]);
        }
        benchmark::DoNotOptimize(a);
    }
}
BENCHMARK(BM_random);

BENCHMARK_MAIN();
```

同样是vector填充 , 随机访问的效率比顺序访问低的多。

其中一个原因当然是：随机访问只会访问到其中一个float，而这导致他附近的64字节都被读取到缓存了，但实际只用到了其中4字节，之后又没用到剩下的60字节，导致浪费了94%的带宽。

那么,如果我们每次把数据分为64字节的数据块,随机读取数据块,而数据块内部还是顺序访问,按道理不会浪费带宽了

然而 , 通过实际运行 , 我们发现运行速度虽然比起随机访问大大提升,但比起顺序访问还是慢一些,为什么?

```cpp
void BM_random_64B(benchmark::State &bm) {
    for (auto _: bm) {
#pragma omp parallel for
        for (size_t i = 0; i < n / 16; i++) {
            size_t r = randomize(i) % (n / 16);
            for (size_t j = 0; j < 16; j++) {
                benchmark::DoNotOptimize(a[r * 16 + j]);
            }
        }
        benchmark::DoNotOptimize(a);
    }
}
BENCHMARK(BM_random_64B);
```

### 缓存行预取技术

当程序顺序访问 a[0], a[1] 时，CPU会智能地预测到你接下来可能会读取 a[2]，于是会**提前给缓存发送一个读取指令**，让他读取 a[2]、a[3]。缓存在后台默默读取数据的同时，CPU自己在继续处理 a[0] 的数据。这样等 a[0], a[1] 处理完以后，缓存也刚好读取完 a[2] 了，从而CPU不用等待，就可以直接开始处理 a[2]，避免等待数据的时候CPU空转浪费时间。

这种策略称之为预取（prefetch），由硬件自动识别你程序的访存规律，决定要预取的地址。一般来说只有线性的地址访问规律（包括顺序、逆序；连续、跨步）能被识别出来，而**如果你的访存是随机的，那就没办法预测**。遇到这种突如其来的访存时，CPU不得不空转等待数据的抵达才能继续工作，浪费了时间。

### 解决方案 : 按更大的分块随机访问

解决方案就是，把分块的大小调的更大一些，比如 4KB 那么大，即64个缓存行，而不是一个。

这样一次随机访问之后会伴随着64次顺序访问，能被CPU检测到，从而启动缓存行预取，避免了等待数据抵达前空转浪费时间。

### 页对齐的重要性

为什么要 4KB？原来现在操作系统管理内存是用分页（page），程序的内存是一页一页贴在地址空间中的，有些地方可能不可访问，或者还没有分配，则把这个页设为不可用状态，访问他就会出错，进入内核模式。

因此硬件出于安全，预取不能跨越页边界，否则可能会触发不必要的 page fault。所以我们选用页的大小，因为本来就不能跨页顺序预取，所以被我们切断掉也无所谓。

另外，我们可以用 _mm_alloc 申请起始地址**对齐到页边界**的一段内存，真正做到每个块内部不出现跨页现象。

```cpp
void BM_random_4KB(benchmark::State &bm) {
    for (auto _: bm) {
#pragma omp parallel for
        for (size_t i = 0; i < n / 1024; i++) {
            size_t r = randomize(i) % (n / 1024);
            for (size_t j = 0; j < 1024; j++) {
                benchmark::DoNotOptimize(a[r * 1024 + j]);
            }
        }
        benchmark::DoNotOptimize(a);
    }
}
BENCHMARK(BM_random_4KB);

void BM_random_4KB_aligned(benchmark::State &bm) {
    float *a = (float *)_mm_malloc(n * sizeof(float), 4096);
    memset(a, 0, n * sizeof(float));
    for (auto _: bm) {
#pragma omp parallel for
        for (size_t i = 0; i < n / 1024; i++) {
            size_t r = randomize(i) % (n / 1024);
            for (size_t j = 0; j < 1024; j++) {
                benchmark::DoNotOptimize(a[r * 1024 + j]);
            }
        }
        benchmark::DoNotOptimize(a);
    }
    _mm_free(a);
}
BENCHMARK(BM_random_4KB_aligned);
```

### 手动预取 : _mm_prefetch

```cpp
void BM_random_64B_prefetch(benchmark::State &bm) {
    for (auto _: bm) {
#pragma omp parallel for
        for (size_t i = 0; i < n / 16; i++) {
            size_t next_r = randomize(i + 64) % (n / 16);
            _mm_prefetch(&a[next_r * 16], _MM_HINT_T0);
            size_t r = randomize(i) % (n / 16);
            for (size_t j = 0; j < 16; j++) {
                benchmark::DoNotOptimize(a[r * 16 + j]);
            }
        }
        benchmark::DoNotOptimize(a);
    }
}
BENCHMARK(BM_random_64B_prefetch);
```

对于不得不随机访问很小一块的情况，还可以通过 _mm_prefetch 指令手动预取一个缓存行。

这里第一个参数是要预取的地址（最好对齐到缓存行），第二个参数 _MM_HINT_T0 代表预取数据到一级缓存，_MM_HINT_T1 代表只取到二级缓存，_MM_HINT_T2 代表三级缓存；_MM_HINT_NTA 则是预取到非临时缓冲结构中，可以最小化对缓存的污染，但是必须很快被用上。

### 重新理解 mem-bound : 延迟隐藏

之前提到，1次浮点读写必须伴随着32次浮点加法的运算量，否则和只有0次加法的耗时没有任何区别，即内存带宽成唯一瓶颈的mem-bound。可是按我们理解，“1次读写+0次加法”应该会比“1次读写+8次加法”快一点点吧，因为8次加法尽管比1次读写快很多，但是毕竟还是有时间的啊，为什么会几乎没有任何区别？

这都是得益于CPU的预取机制，他能够在等待a[i+1]的内存数据抵达时，默默地做着a[i]的计算，从而只要计算的延迟小于内存的延迟，延迟就被隐藏起来了，而不必等内存抵达了再算。这就是为什么有些运算量不足32次的程序还是会无法达到mem-bound，手动预取以后才能达到，就是因为硬件预取预测失败，导致不得不等内存抵达了才能算，导致延迟隐藏失败。

隐藏成功：

![image-20220228150524935](.\img\image-20220228150524935.png)

### 写入的粒度太小造成不必要的读取

如果运行以下代码

```cpp
void BM_read(benchmark::State &bm) {
    for (auto _: bm) {
        float ret = 0;
#pragma omp parallel for reduction(+:ret)
        for (size_t i = 0; i < n; i++) {
            ret += a[i];
        }
        benchmark::DoNotOptimize(ret);
        benchmark::DoNotOptimize(a);
    }
}
BENCHMARK(BM_read);

void BM_write(benchmark::State &bm) {
    for (auto _: bm) {
#pragma omp parallel for
        for (size_t i = 0; i < n; i++) {
            a[i] = 1;
        }
        benchmark::DoNotOptimize(a);
    }
}
BENCHMARK(BM_write);

void BM_read_and_write(benchmark::State &bm) {
    for (auto _: bm) {
#pragma omp parallel for
        for (size_t i = 0; i < n; i++) {
            a[i] = a[i] + 1;
        }
        benchmark::DoNotOptimize(a);
    }
}
BENCHMARK(BM_read_and_write);
```

![image-20220228150844424](.\img\image-20220228150844424.png)

不难发现 : 写入花的时间似乎是读取的2倍 , 而且写入的同时读取 , 和单单写入所用的时间是一样的 , 似乎写入一个数组的同时也会读取这个数组，造成两倍带宽

这是因为缓存和内存通信的最小单位是缓存行：64字节。

当CPU试图写入4字节时，因为剩下的60字节没有改变，缓存不知道CPU接下来会不会用到那60字节，因此他只好从内存读取完整的64字节，修改其中的4字节为CPU给的数据，之后再择机写回。

这就导致了虽然没有用到读取数据，但实际上缓存还是从内存读取了，从而浪费了2倍带宽。  

### 解决方案 : 绕过缓存 , 直接写入

因此需要把16次float用SIMD指令合并成一次写入，且写入的地址要对齐到64字节，才能避免浪费读取的带宽。这样的条件实在有点苛刻

可以用 _mm_stream_si32 指令代替直接赋值的写入，他能够**绕开缓存**，将一个4字节的写入操作，挂起到临时队列，等凑满64字节后，直接写入内存，从而完全避免读的带宽。

但是这个指令只支持int做参数，要用float还得转换一下指针类型，bitcast一下参数。

```cpp
void BM_write_streamed(benchmark::State &bm) {
    for (auto _: bm) {
#pragma omp parallel for
        for (size_t i = 0; i < n; i++) {
            float value = 1;
            _mm_stream_si32((int *)&a[i], *(int *)&value);
        }
        benchmark::DoNotOptimize(a);
    }
}
BENCHMARK(BM_write_streamed);
```

因为 _mm_stream_si32 会绕开缓存，直接把数据写到内存，之后读取的话，反而需要等待 stream 写回执行完成，然后重新读取到缓存，反而更低效。

因此，仅当这些情况才应该用 stream 指令 : 

1. 该数组只有写入，之前完全没有读取过。

2. 之后没有再读取该数组的地方。

需要注意，stream 系列指令写入的地址，必须是连续的，中间**不能有跨步**，否则无法合并写入，会产生有中间数据读的带宽。

### 四倍矢量化版本 : _mm_stream_ps

_mm_stream_si32 可以一次性写入4字节到挂起队列。而 _mm_stream_ps 可以一次性写入 16 字节到挂起队列，更加高效了。

他的第二参数是一个 __m128 类型，可以配合其他手写的 SIMD 指令使用。

不过，_mm_stream_ps 写入的地址必须对齐到 16 字节，否则会产生段错误等异常。

```cpp
void BM_write_streamed_ps(benchmark::State &bm) {
    for (auto _: bm) {
#pragma omp parallel for
        for (size_t i = 0; i < n; i += 4) {
            _mm_stream_ps(&a[i], _mm_set1_ps(1.f));
        }
        benchmark::DoNotOptimize(a);
    }
}
BENCHMARK(BM_write_streamed_ps);
```

### 回答intro

为什么写入0比写入1更快?

因为写入0被编译器自动优化成了memset，而memset内部利用了stream指令得以更快写入。

优化写入1的方法很简单,我们手动也使用stream就可以了

```cpp
void BM_write1_streamed(benchmark::State &bm) {
    for (auto _: bm) {
#pragma omp parallel for
        for (size_t i = 0; i < n; i++) {
            _mm_stream_si32(&a[i], 1);
        }
        benchmark::DoNotOptimize(a);
    }
}
BENCHMARK(BM_write1_streamed);
```

### 来源

_mm系列指令来自<xmmintrin.h>头文件

指令的文档可以看这个网站：

https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html

里面有详细说明每个指令对应的汇编，方便理解的伪代码，延迟和花费的时钟周期等。

## 循环合并法

### 两个循环体

```cpp
void BM_original(benchmark::State &bm) {
    for (auto _: bm) {
#pragma omp parallel for
        for (size_t i = 0; i < n; i++) {
            a[i] = a[i] * 2;
        }
#pragma omp parallel for
        for (size_t i = 0; i < n; i++) {
            a[i] = a[i] + 1;
        }
        benchmark::DoNotOptimize(a);
    }
}
BENCHMARK(BM_original);
```

原始的代码第一个循环体执行 a[i] = a[i] * 2，等乘法全部结束了以后，再来一个循环体执行 a[i] = a[i] + 1。

因为第一遍循环过了 1GB 的数据，执行到 a[n-1] 时，原本 a[0] 处的缓存早已失效，因此第二遍循环开始读取 a[0] 时必须重新从主内存读取，然后再次写回主内存。

这种代码在主内存看来，CPU做的事情相当于：读+写+读+写，每个元素都需要访问四遍内存。

### 循环合并

```cpp
void BM_optimized(benchmark::State &bm) {
    for (auto _: bm) {
#pragma omp parallel for
        for (size_t i = 0; i < n; i++) {
            a[i] = a[i] * 2;
            a[i] = a[i] + 1;
        }
        benchmark::DoNotOptimize(a);
    }
}
BENCHMARK(BM_optimized);
```

优化后的代码在同一个循环体里，执行完 a[i] = a[i] * 2 后，立即执行了 a[i] = a[i] + 1。

因为执行完 a[i] = a[i] * 2 后不会立即被写回主内存，仍储存在高速缓存里一段时间。这时紧接着执行的 a[i] = a[i] + 1 又用到了 a[i]，所以能够直接从缓存里获取 a[i] 的值。计算出 a[i] + 1 后又存到 a[i]，这次也是先存到高速缓存里，不过当访问到 a[i + 12*1024*1024] 时，我们的缓存装不下了，不得不把之前存储的 a[i] 写回主内存。

这种代码在主内存看来，CPU做的事情相当于：读+写，从而每个元素只需要访问两遍内存。对这种完全 mem-bound 的程序而言就是加速了 2 倍。

可见,能否很好的利用缓存,和程序访问内存的**时间局域性**有关

### 案例 : 一维jacobi迭代

一些物理仿真中，常用到这种形式的迭代法：

```cpp
for (i=0...n) b[i] = a[i + 1] + a[i - 1];  // 假装是jacobi
swap(a, b);   // 交换双缓冲
for (i=0...n) b[i] = a[i + 1] + a[i - 1];  // 假装是jacobi
swap(a, b);   // 交换双缓冲
// 不断反复...
```

但是这样每个循环体内只有 1 次加法，明显就是我们所说的 mem-bound 嘛！

一种简单的优化方法是 : 

既然递推公式 a’[i] = (a[i - 1] + a[i + 1]) * 0.5

那么也应该有 a’’[i] = (a’[i - 1] + a’[i + 1]) * 0.5

不妨带入(1)式到(2)式，得到：

a’’[i] = (a[i - 2] + a[i + 2]) * 0.25 + a[i] * 0.5

我们得到了求出两次迭代后状态的公式。这样就可以在一个循环体内实现两次迭代的效果！从而快了 2 倍。

#### 再优化 : 局部数组,一步抵十六步

一次性读取到局部数组ta里，在局部迭代16次。

注意到局部数组是64大小，这包含了中心的**32**个元素，还包含因为jacobi特性需要周围两个元素，导致迭代16次就需要往边缘扩张的**16**个元素。

因为局部数组的大小远远小于一级缓存，这样迭代时读写的带宽就是一级缓存的速度，几乎没有影响。

这里一次循环体直接相当于16次迭代

例程

```cpp
#include <iostream>
#include <vector>
#include <cmath>
#include <cstring>
#include <cstdlib>
#include <array>
#include "ticktock.h"
#include "mtprint.h"
#include <x86intrin.h>
#include <omp.h>

// L1: 32KB
// L2: 256KB
// L3: 12MB

constexpr size_t n = 1<<26;
constexpr size_t steps = 32/16;

std::vector<float> a(n);  // 256MB
std::vector<float> b(n);

int main() {
#pragma omp parallel for //这个是OpenMP框架的指令 表明接下来的for循环将会并行执行
    for (size_t i = 0; i < n; i++) {
        a[i] = std::sin(i * 0.1f);
    }
    for (int step = 0; step < steps; step++) {
#pragma omp parallel for
        for (size_t ibase = 16; ibase < n - 16; ibase += 32) {
            float ta[32 + 16 * 2], tb[32 + 16 * 2];
            for (intptr_t i = -16; i < 32 + 16; i++) {
                ta[16 + i] = a[ibase + i];
            }
            for (intptr_t s = 1; s < 16; s += 2) {
                for (intptr_t i = -16 + s; i < 32 + 16 - s; i++) {
                    tb[16 + i] = (ta[16 + i - 1] + ta[16 + i + 1]) * 0.5f;
                }
                for (intptr_t i = -16 + s + 1; i < 32 + 16 - s - 1; i++) {
                    ta[16 + i] = (tb[16 + i - 1] + tb[16 + i + 1]) * 0.5f;
                }
            }
            for (intptr_t i = 0; i < 32; i++) {
                b[ibase + i] = tb[16 + i];
            }
        }
        std::swap(a, b);
    }
    float loss = 0;
#pragma omp parallel for reduction(+:loss)
    for (size_t i = 1; i < n - 1; i++) {
        loss += std::pow(a[i - 1] + a[i + 1] - a[i] * 2, 2);
    }
    loss = std::sqrt(loss);
    std::cout << "loss: " << loss << std::endl;
    return 0;
}
```

#### 再再优化

把常量作为参数 , 调整局部数组的大小

```cpp
    for (int step = 0; step < steps; step++) {
        constexpr intptr_t BS = 128;
        constexpr intptr_t HS = 16;
#pragma omp parallel for
        for (intptr_t ibase = HS; ibase < n - HS; ibase += BS) {
            float ta[BS + HS * 2], tb[BS + HS * 2];
            for (intptr_t i = -HS; i < BS + HS; i++) {
                ta[HS + i] = a[ibase + i];
            }
#pragma GCC unroll 8
            for (intptr_t s = 1; s < HS; s += 2) {
#pragma omp simd
                for (intptr_t i = -HS + s; i < BS + HS - s; i++) {
                    tb[HS + i] = (ta[HS + i - 1] + ta[HS + i + 1]) * 0.5f;
                }
#pragma omp simd
                for (intptr_t i = -HS + s + 1; i < BS + HS - s - 1; i++) {
                    ta[HS + i] = (tb[HS + i - 1] + tb[HS + i + 1]) * 0.5f;
                }
            }
            for (intptr_t i = 0; i < BS; i++) {
                b[ibase + i] = tb[HS + i];
            }
        }
        std::swap(a, b);
    }
```

#### 再再再优化 : 用stream_ps防止写回操作污染缓存

加速比达到了24倍

```cpp
    for (int step = 0; step < steps; step++) {
        constexpr intptr_t BS = 128;
        constexpr intptr_t HS = 16;
#pragma omp parallel for
        for (intptr_t ibase = HS; ibase < n - HS; ibase += BS) {
            float ta[BS + HS * 2], tb[BS + HS * 2];
            for (intptr_t i = -HS; i < BS + HS; i++) {
                ta[HS + i] = a[ibase + i];
            }
#pragma GCC unroll 8
            for (intptr_t s = 2; s <= HS; s += 4) {
#pragma omp simd
                for (intptr_t i = -HS + 2; i < BS + HS - 2; i++) {
                    tb[HS + i] = (ta[HS + i - 2] + ta[HS + i] + ta[HS + i] + ta[HS + i + 2]) * 0.25f;
                }
#pragma omp simd
                for (intptr_t i = -HS + 4; i < BS + HS - 4; i++) {
                    ta[HS + i] = (tb[HS + i - 2] + tb[HS + i] + tb[HS + i] + tb[HS + i + 2]) * 0.25f;
                }
            }
            for (intptr_t i = 0; i < BS; i += 4) {
                _mm_stream_ps(&b[ibase + i], _mm_loadu_ps(&tb[HS + i]));	//这里!!!
            }
        }
        std::swap(a, b);
    }
```

## 内存分配和分页

### Intro

如果我们把一个vector写入两次,发现用时一样的

```cpp
int main(){
	std::vector<int> arr(n);
	
	for(size_t i = 0;i < n;i++) {
		arr[i] = 1;
	}
	
	for(size_t i = 0;i < n;i++) {
		arr[i] = 1;
	}
    
    return 0;
}
```

但是如果我们使用malloc写入两次,却会发现第一次比第二次慢

```cpp
int main() {
	int *arr = (int *)malloc(n * sizeof(int));
    
	for(size_t i = 0;i < n;i++) {
		arr[i] = 1;
	}
	
	for(size_t i = 0;i < n;i++) {
		arr[i] = 1;
	}
    
    free(arr);
	return 0;
}
```

换成new int[N],发现和malloc是一样的, 后面加上{}变成new int[N]{}就和vector一样了

### 原理

当调用 malloc 时，操作系统并不会实际分配那一块内存，而是将这一段内存标记为“不可用”。当用户试图访问（写入）这一片内存时，硬件就会触发所谓的缺页中断（page fault），进入操作系统内核，内核会查找当前进程的 malloc 历史记录。如果发现用户写入的地址是他曾经 malloc 过的地址区间，则执行实际的内存分配，并标记该段内存为“可用”，下次访问就不会再产生缺页中断了；而如果用户写入的地址根本不是他 malloc 过的地址，那就说明他确实犯错了，就抛出段错误（segmentation fault）。

std::vector\<int>、new int[n]{} **会**初始化数组为0。

malloc(n * sizeof(int))、new int[n] **不会**初始化数组为0。

初始化数组时，内存被写入，所以操作系统这时候才开始实际分配内存。

刚才的案例里，不会初始化的 malloc，第一次往里面赋值时，因为这时操作系统还没有给这个数组分配内存，所以会触发缺页中断，进入操作系统内核给数组分配内存，是内核执行内存分配的这个动作，花费了额外的时间。而第二次因为内存已经被分配上了，所以再次访问也不会触发缺页中断，所以看起来比第一次快很多。

### 进一步 : 分配是按页面(4KB)来管理的

当一个尚且处于“不可用”的 malloc 过的区间被访问，操作系统不是把整个区间全部分配完毕，而是只把当前写入地址所在的页面（4KB 大小）给分配上。也就是说用户访问 a[0] 以后只分配了 4KB 的内存。等到用户访问了 a[1024]，也就是触及了下一个页面，他才会继续分配一个 4KB 的页面，这时才 8KB 被实际分配, 所以非常快。

### tbbmalloc

tbb提供的tbbmalloc不仅效率比malloc略高, 而且tbb::cache_aligned_allocator 的最大好处在于他分配的内存地址，永远会对齐到缓存行（64字节），对 SIMD 而言可以用 _mm_load_ps 而不是 _mm_loadu_ps 了。对访存优化而言则意味着可以放心地用 _mm_prefetch，也更高效。

```cpp
std::vector<int, tbb::cache_aligned_allocator<int>> arr(n);
```

不过其实标准库的 new 和 malloc 已经可以保证 16 字节对齐了。如果你只需要用 _mm_load_ps 而不用 _mm256_load_ps 的话，那直接用标准库的内存分配也没问题

还有 _mm_malloc(n, aalign) 可以分配对齐到任意 a 字节的内存。他在 <xmmintrin.h> 这个头文件里。是 x86 特有的，并且需要通过 _mm_free 来释放。

还有一个跨平台版本（比如用于 arm 架构）的 aligned_alloc(align, n)，他也可以分配对齐到任意 a 字节的内存，通过 free 释放。

利用他们可以实现分配对齐到页面（4KB）的内存。

```cpp
constexpr size_t n = 1<<20;

int main() {
    std::cout << std::boolalpha;
    for (int i = 0; i < 5; i++) {
        auto arr = (int *)_mm_malloc(n * sizeof(int), 4096);
        bool is_aligned = (uintptr_t)arr % 4096 == 0;
        std::cout << "_mm_malloc: " << is_aligned << std::endl;
        _mm_free(arr);
    }
    for (int i = 0; i < 5; i++) {
        auto arr = (int *)aligned_alloc(4096, n * sizeof(int));
        bool is_aligned = (uintptr_t)arr % 4096 == 0;
        std::cout << "aligned_alloc: " << is_aligned << std::endl;
        free(arr);
    }
    return 0;
}
```

### 内存池化

```cpp
float func(int n) {
    std::vector<float> tmp;
    for (int i = 0; i < n; i++) {
        tmp.push_back(i / 15 * 2.71828f);
    }
    std::reverse(tmp.begin(), tmp.end());
    float ret = tmp[32];
    return ret;
}
```

临时创建的数组，每次调用 func 都会重复内存分配一次（进入一次内核态），非常浪费时间。

* 解决方案 : 

  ```cpp
  float func(int n) {
      static thread_local std::vector<float> tmp;
      for (int i = 0; i < n; i++) {
          tmp.push_back(i / 15 * 2.718f);
      }
      std::reverse(tmp.begin(), tmp.end());
      float ret = tmp[32];
      tmp.clear();
      return ret;
  }
  ```

声明为 static 变量，这样第二次进入 func 的时候还是同一个数组，不需要重复分配内存。thread_local 表示如有多个线程，每个线程保留一个 tmp 对象的副本，防止多线程调用 func 出错。

返回时（或者进入时）调用 tmp.clear() 清除已有数据。由于 vector 的特性，他只会把 size() 标记为 0 并调用其成员的解构函数，而不会实际释放内存（free）。

因此第二次进入的时候，如果 n 不超过上一次的大小，就还是用的第一次分配的内存，避免了重新分配的开销。对 func 需要被重复调用的情况很实用。

## 多维数组

